# ğŸš— SmartInsureAI

**An Intelligent Pipeline for Vehicle Insurance Risk Assessment**  
Capstone Project | Prepared by *Islam Nabi*

---

## ğŸ“Œ Overview

SmartInsureAI is a robust and production-grade **MLOps project** built to automate the entire machine learning lifecycle for **vehicle insurance risk assessment**. It showcases how to manage real-world data, build and deploy ML models, and automate the CI/CD process using modern DevOps and cloud infrastructure.

---

## ğŸ§  Problem Statement

Insurance providers often rely on manual, outdated processes for risk assessment, which leads to inefficiencies, fraud, and poor scalability. Moreover, many machine learning solutions fail due to inadequate deployment and lack of automation.

---

## âœ… Proposed Solution

SmartInsureAI solves these challenges by building a **complete end-to-end MLOps pipeline** that:

- Ingests and stores vehicle insurance data in **MongoDB Atlas**
- Validates and transforms data using defined schemas
- Trains models to predict **claim likelihood or customer risk**
- Deploys a **FastAPI** for real-time predictions
- Stores models in **AWS S3** and hosts the API on **AWS EC2**
- Implements **CI/CD pipelines** using Docker and GitHub Actions
- Enables full traceability, reproducibility, and deployment automation

---

## ğŸ§° Tech Stack

| Category        | Tools/Technologies                            |
|----------------|-----------------------------------------------|
| Language        | Python 3.10                                   |
| ML Libraries    | Scikit-learn, Pandas, NumPy                   |
| Web Framework   | FastAPI                                        |
| Database        | MongoDB Atlas                                 |
| Cloud           | AWS S3, EC2, ECR                              |
| DevOps          | Docker, GitHub Actions, GitHub Secrets        |
| Utilities       | Logging, YAML Configs, Exception Handling     |

---

## ğŸ› ï¸ Project Structure

SmartInsureAI/
â”‚
â”œâ”€â”€ src/ # Core source code
â”‚ â”œâ”€â”€ components/ # Modular pipeline components (ingestion, training)
â”‚ â”œâ”€â”€ entity/ # Data classes: config_entity.py, artifact_entity.py
â”‚ â”œâ”€â”€ config/ # schema.yaml, DB configs
â”‚ â”œâ”€â”€ utils/ # Utility functions (validation, transformation)
â”‚ â”œâ”€â”€ aws_storage/ # S3 upload/download handlers
â”‚
â”œâ”€â”€ api/ # FastAPI
â”‚ â”œâ”€â”€ app.py # Main entry point for prediction
â”‚ â”œâ”€â”€ templates/ # Web UI templates
â”‚ â””â”€â”€ static/ # Static files (CSS/JS/images)
â”‚
â”œâ”€â”€ notebook/ # EDA notebooks and MongoDB push demo
â”œâ”€â”€ tests/ # Unit and integration tests
â”œâ”€â”€ Dockerfile # Docker build instructions
â”œâ”€â”€ requirements.txt # Project dependencies
â”œâ”€â”€ setup.py / pyproject.toml # Package setup files
â”œâ”€â”€ template.py # Project initializer script
â””â”€â”€ README.md # Project documentation

---

## ğŸ§© Step-by-Step Implementation

### 1ï¸âƒ£ Project Initialization

- Run `template.py` to create folder structure
- Set up `setup.py` and `pyproject.toml` for local imports
- Create a virtual environment:
  ```bash
  conda create -n vehicle python=3.10 -y
  conda activate vehicle
  pip install -r requirements.txt

2ï¸âƒ£ MongoDB Atlas Integration
Set up MongoDB Atlas cluster

Push dataset to MongoDB using mongoDB_demo.ipynb

Verify collections via the MongoDB UI

3ï¸âƒ£ Logging, Exception Handling & EDA
Implement logging & exception modules

Run test via demo.py

Perform EDA and feature engineering in the notebook/

4ï¸âƒ£ Data Ingestion Pipeline
Use components.data_ingestion.py and data_access/

Configure ingestion settings in config_entity.py and artifact_entity.py

MongoDB URL setup:
export MONGODB_URL="mongodb+srv://<username>:<password>@cluster.mongodb.net/"

5ï¸âƒ£ Data Validation & Transformation
Define schema in config/schema.yaml

Write validation logic in utils.main_utils.py

Transform features in components.data_transformation.py

6ï¸âƒ£ Model Training
Define model logic in components.model_trainer.py

Use estimators defined in entity/estimator.py

Evaluate with cross-validation and key metrics

7ï¸âƒ£ Model Evaluation & AWS S3 Integration
Set up AWS IAM user and access keys

Store credentials:

bash
Copy
Edit
export AWS_ACCESS_KEY_ID="your_key"
export AWS_SECRET_ACCESS_KEY="your_secret"
Push model to S3://my-model-mlopsproj (region: us-east-1)

Code resides in aws_storage/ and entity/s3_estimator.py

8ï¸âƒ£ Prediction API with Flask
Implement API logic in api/app.py

Load model from S3 and serve predictions

Add frontend HTML in templates/

9ï¸âƒ£ CI/CD with Docker & GitHub Actions
Create Dockerfile and .dockerignore

Set GitHub Secrets:

AWS_ACCESS_KEY_ID

AWS_SECRET_ACCESS_KEY

ECR_REPO, AWS_DEFAULT_REGION

Automate:

Build â†’ Test â†’ Push â†’ Deploy

Via main.yml GitHub Actions

ğŸ”Ÿ AWS EC2 & Self-Hosted Runner Setup
Launch EC2 instance

Open port 5080 in the security group

Install Docker and run as GitHub self-hosted runner

Access app via http://<EC2_PUBLIC_IP>:5080

ğŸ“Š Evaluation Metrics
Category	Metrics Used
Model	Accuracy, Precision, Recall, F1, ROC-AUC
Pipeline	Unit Tests, Integration Tests
Deployment	GitHub Action Logs, API Health Check

ğŸŒ Final Project Workflow
Data Ingestion â†’ Validation â†’ Transformation â†’
Model Training â†’ Evaluation â†’ Deployment â†’
CI/CD Automation â†’ Live API via EC2


